{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install weaviate-client langchain tiktoken pypdf rapidocr-onnxruntime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weaviate :\n",
    "-  it is vector database similar to faiss and pinecone\n",
    "\n",
    "It provides built-in features for data management, such as data ingestion, indexing, and query processing, all via API endpoints.\n",
    "\n",
    "\n",
    "\n",
    "Weaviate can store data both locally and in the cloud, depending on how it is deployed and configured\n",
    "\n",
    "\n",
    "`1. Local Storage:`\n",
    "\n",
    "Local Deployment: When you run Weaviate on your local machine or on-premises servers, it stores data on the local filesystem. This is typically done using local disk storage, and the data resides on the hardware where Weaviate is installed.\n",
    "File System and Disk Storage: Weaviate uses local disk space to store both vector embeddings and any associated metadata. This can include storing vector indexes, schema definitions, and other data objects.\n",
    "Use Case: Local storage is suitable for development, testing, or scenarios where you want full control over data management and storage without relying on external cloud services.\n",
    "\n",
    "\n",
    "`2. Cloud Storage:`\n",
    "\n",
    "Cloud Deployment: Weaviate is designed to be cloud-native, which means it can be deployed on cloud infrastructure such as AWS, Google Cloud Platform (GCP), Microsoft Azure, and other cloud providers.\n",
    "Containerization and Kubernetes: Weaviate can be deployed using Docker containers or orchestrated with Kubernetes. In a cloud environment, these containers can utilize cloud storage solutions like AWS EBS (Elastic Block Store), Google Cloud Persistent Disks, or Azure Managed Disks for data persistence.\n",
    "Object Storage Integration: Weaviate can integrate with cloud object storage services (e.g., AWS S3, Google Cloud Storage, Azure Blob Storage) to handle backups, snapshots, or large data sets that are stored externally and accessed on demand.\n",
    "Managed Cloud Services: There are also managed Weaviate services available that handle cloud storage and scalability for you, abstracting away the need to configure storage manually.\n",
    "\n",
    "\n",
    "\n",
    "`3. Hybrid Storage Configurations:`\n",
    "\n",
    "Hybrid Deployment: You can configure Weaviate to use both local storage and cloud storage simultaneously. For example, vector embeddings might be stored locally for fast access, while backup data or less frequently accessed information is stored in cloud storage.\n",
    "Data Replication and Backup: Weaviate can use cloud storage for data replication and backup strategies, ensuring that data is safely stored off-site and can be restored if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import Weaviate\n",
    "# !pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\venv\\lib\\site-packages\\weaviate\\__init__.py:143: DeprecationWarning: Dep010: Importing AuthApiKey from weaviate is deprecated. Import AuthApiKey from its module: weaviate.auth\n",
      "  _Warnings.root_module_import(name, map_[name])\n",
      "d:\\study\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\venv\\lib\\site-packages\\weaviate\\warnings.py:162: DeprecationWarning: Dep016: Python client v3 `weaviate.Client(...)` connections and methods are deprecated. Update\n",
      "            your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "\n",
      "            For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from langchain.vectorstores import Weaviate\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "\n",
    "\n",
    "\n",
    "client = weaviate.Client(\n",
    "    url=WEAVIATE_URL, auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY)\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\study\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence-transformers\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "  model_name=embedding_model_name,\n",
    "  # model_kwargs=model_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load multiple types of pdf using the langchain just check with the document\n",
    "\n",
    "'https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(r\"D:\\study\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\sensors.pdf\", extract_images=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 0}, page_content='Citation: Takenaka, K.; Kondo, K.;\\nHasegawa, T. Segment-Based\\nUnsupervised Learning Method in\\nSensor-Based Human Activity\\nRecognition. Sensors 2023 ,23, 8449.\\nhttps://doi.org/10.3390/s23208449\\nAcademic Editor: Eui Chul Lee\\nReceived: 24 August 2023\\nRevised: 22 September 2023\\nAccepted: 11 October 2023\\nPublished: 13 October 2023\\nCopyright: © 2023 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\nsensors\\nArticle\\nSegment-Based Unsupervised Learning Method in Sensor-Based\\nHuman Activity Recognition\\nKoki Takenaka *, Kei Kondo†and Tatsuhito Hasegawa\\nGraduate School of Engineering, University of Fukui, Fukui 910-8507, Japan; kicont115sub@gmail.com (K.K.);\\nt-hase@u-fukui.ac.jp (T.H.)\\n*Correspondence: ktakenak@u-fukui.ac.jp\\n†Current address: FLECT Co., Ltd., Tokyo 105-0023, Japan.\\nAbstract: Sensor-based human activity recognition (HAR) is a task to recognize human activities,\\nand HAR has an important role in analyzing human behavior such as in the healthcare ﬁeld. HAR is\\ntypically implemented using traditional machine learning methods. In contrast to traditional machine\\nlearning methods, deep learning models can be trained end-to-end with automatic feature extraction\\nfrom raw sensor data. Therefore, deep learning models can adapt to various situations. However,\\ndeep learning models require substantial amounts of training data, and annotating activity labels\\nto construct a training dataset is cost-intensive due to the need for human labor. In this study, we\\nfocused on the continuity of activities and propose a segment-based unsupervised deep learning\\nmethod for HAR using accelerometer sensor data. We deﬁne segment data as sensor data measured\\nat one time, and this includes only a single activity. To collect the segment data, we propose a\\nmeasurement method where the users only need to annotate the starting, changing, and ending\\npoints of their activity rather than the activity label. We developed a new segment-based SimCLR,\\nwhich uses pairs of segment data, and propose a method that combines segment-based SimCLR with\\nSDFD. We investigated the effectiveness of feature representations obtained by training the linear\\nlayer with ﬁxed weights obtained by unsupervised learning methods. As a result, we demonstrated\\nthat the proposed combined method acquires generalized feature representations. The results of\\ntransfer learning on different datasets suggest that the proposed method is robust to the sampling\\nfrequency of the sensor data, although it requires more training data than other methods.\\nKeywords: human activity recognition; unsupervised representation learning; accelerometer sensor\\ndata; segment data\\n1. Introduction\\nHuman activity recognition (HAR) using sensor data is a task that recognizes human\\nactivities wherein the data are measured by sensors such as accelerometers and gyroscopes.\\nFor example, human activities such as “walking” and “running” were predicted from\\nthe measured accelerometer data. This technology is used in various applications from\\nanalyzing sports movements [ 1] to healthcare such as health awareness maintenance and\\nthe detection of risky movements by patients [ 2–4]. HAR is an essential technology because\\nthe predicted results have some inﬂuence on decision-making.\\nRecently, neural networks (NNs) have been used for HAR [ 5–8]. HAR is typically\\nimplemented using machine learning methods [ 9,10] such as support vector machines\\n(SVMs) and hidden Markov models (HMMs). Machine learning models require a training\\ndataset consisting of handcrafted features extracted from sensor data and corresponding\\nactivity labels. In contrast to traditional machine learning, deep learning methods such\\nas NNs have the advantage of automatic feature extraction, which enables end-to-end\\ntraining and prediction from raw sensor data. Therefore, deep learning models can adapt\\nto various situations [ 11] without requiring domain expertise for feature extraction. NNs\\nSensors 2023 ,23, 8449. https://doi.org/10.3390/s23208449 https://www.mdpi.com/journal/sensors'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 1}, page_content='Sensors 2023 ,23, 8449 2 of 15\\nrequire substantial amounts of training data to acquire generalized feature representations\\nfrom sensor data compared to traditional methods. In recent years, the widespread use of\\nsmartphones and wearable devices has made it easy to measure sensor data. However, the\\nannotation process is time-consuming and labor-intensive to generate a large amount of\\ntraining data. Thus, there is a need for an unsupervised learning method for NNs.\\nIn the case of sensor-based HAR, we can design how to measure the sensor data for the\\ntraining dataset. For example, there are two measurement methods for HAR: the sequential\\nmeasurement of multiple activities and the individual measurement of each activity. When\\neach activity is measured individually without annotation, the accurate activity of the\\nmeasurement data is unclear (annotator needs to determine the activity label using video,\\netc.) However, it can be understood that each of the data corresponds to a single activity.\\nOur purpose was to develop an unsupervised learning method that leverages char-\\nacteristics implicitly possessed by sensor data for sensor-based HAR using deep learning\\nmethods. Existing unsupervised learning techniques for time series data are based on time\\nseries characteristics [ 12,13] or masks [ 14], but these methods do not focus on the structure\\nof the training data. Xiao et al. [ 15] proposed a method employing contrastive loss. This\\napproach calculates the loss in the intermediate layers, and it is not entirely independent of\\nthe model’s structure. Franceschi et al. [ 16] introduced triplet loss (T-Loss), which selects\\npositive samples from the same subseries as a given time series and randomly selects nega-\\ntive samples from other subseries. When utilizing segmented data, this selection method\\ninvolves considering the data structure, where positive samples should be chosen from the\\nsame segment and negative samples should be chosen from different segments. SimCLR\\nemploys the normalized temperature-scaled cross-entropy (NT-Xent) loss [ 17,18], an ex-\\npanded loss function that is derived from the triplet loss by incorporating multiple negative\\nsamples. As this loss function is compatible with the approach of Franceschi et al. [16] ,\\nthere is room for improvement in utilizing segmented data. We investigated a method that\\nfocuses on the data structure for unsupervised learning with discriminators [ 19]. In this\\nstudy, we introduced SimCLR, which performs contrastive feature representation based on\\nsegments, to segment discrimination and feature decorrelation (SDFD) [ 19]. We combined\\nthese methods to develop a sensor-based unsupervised HAR learning method that does\\nnot rely on model structure, but focuses on the structure of segmented data. We refer\\nto segment-based SimCLR as SimCLR (seg) and the combined method segment-based\\nSimCLR with SDFD as SimCLR (seg) + SDFD. In our experiments, in order to evaluate\\nthe generalization performance of the proposed method, we conducted a linear evaluation\\nunder the two task settings: transfer learning within the same domain and transfer learning\\nacross different domains.\\nIn summary, our contributions are as follows:\\n• We propose a new unsupervised learning method, SimCLR (seg) + SDFD, for sensor-\\nbased HAR in an environment where the training dataset consists of segmented data.\\nIn the proposed method, segmented data are used only during training and are not\\nused during the evaluation.\\n• The ablation study showed that SimCLR (seg) and SDFD improved the f1-score\\ncompared to methods that do not focus on the data structure. This shows that focusing\\non the structure of the training dataset is effective.\\n• We demonstrated that our proposed method acquires generalized feature representa-\\ntions for sensor-based HAR through our experiments.\\n2. Related Works\\nIn sensor-based HAR, models are typically trained using data consisting of sensor\\ndata and corresponding activity labels. Liu et al. [ 10] and Hartmann et al. [ 20] worked on\\nreal-time HAR using HMMs, and Liu et al. [ 21] demonstrated that HMMs are computation-\\nally efﬁcient and fully adaptable to real-time applications. Bento et al. [ 22] demonstrated\\nthat traditional machine learning methods using handcrafted features of sensor data can\\ngeneralize to data from different domains than the training data. These traditional methods'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 2}, page_content='Sensors 2023 ,23, 8449 3 of 15\\nrequire manual feature extraction, whereas deep learning methods extract features auto-\\nmatically. Thus, deep learning methods can be trained and predicted end-to-end and have\\ndemonstrated their adaptive capabilities [11].\\nFor sensor-based HAR using deep learning methods, various methods using con-\\nvolutional NNs (CNNs) [ 5–8] and transformers [ 23] are proposed. Mahmud et al. [ 24]\\nproposed a method designed to acquire useful features for HAR from different perspectives\\nby applying multiple transformation methods to sensor data. Sanchez et al. [ 8] proposed a\\nmethod that converts sensor data into an image format for HAR. These studies focused on\\nan input format for sensor data to improve recognition accuracy. Approaches to improve\\nthe architecture of NNs for HAR have been proposed, such as a method that uses multiple\\nmodels with different roles [ 25] and another that uses attention mechanisms [ 26]. Some\\nstudies tackle representation learning with activity labels [ 27,28]. These studies focus\\non acquiring feature representations for HAR using datasets with activity labels. Deep\\nlearning models require more training data for automatic feature extraction [ 24,29]. In\\nrecent years, the widespread use of smartphones and wearable devices has enabled the\\ncollection of large amounts of sensor data. However, annotators need to manually annotate\\nactivity labels for each of the measurement data, which requires more-signiﬁcant human\\nresources and time. On the other hand, this study focuses on learning methods without\\nactivity labels, which in turn reduces annotation costs.\\nIn image recognition, several unsupervised learning methods have been proposed [30,31] .\\nTao et al. [ 32] proposed a clustering-friendly representation learning method by utilizing\\ninstance discrimination (ID) [ 33], which classiﬁes each of the data as a unique class and\\nremoves the redundant correlation of features. Chen et al. [ 18] proposed SimCLR as a con-\\ntrastive learning method that acquires feature representations by a straightforward method.\\nGrill et al. [34] proposed bootstrap your own latent (BYOL) as an efﬁcient representation\\nlearning method without using negative examples. Since these methods do not depend on\\na model architecture, they can be applied to unsupervised learning methods with sensor\\ndata by changing an encoder architecture into layers for sensor data.\\nMethods for segmenting unlabeled sensor data have been proposed. Rodrigues et al. [35]\\nproposed a method for detecting the similarity between continuous time series data. Folgado\\net al. [36] utilized query sequences for data segmentation. These methods require continuous\\nmeasurement data or query sequences to compute the similarity and features of time series\\ndata. Our approach, on the other hand, allows the model trained with our method to\\ncompute features from short-term measurement data without the need to retain training\\ndata during inference. Furthermore, these methods focus on unsupervised segmentation\\nof time series data. Our approach focuses on training with segmented unsupervised data,\\nand the trained model can be applied to other classiﬁcation tasks using short-term data.\\nMoreover, unsupervised learning methods for HAR using deep learning methods\\nhave been proposed to acquire feature representations from unlabeled sensor data [ 16,29].\\nFocusing on time series, Haresamudram et al. [ 12] proposed a method to use contrastive\\npredictive coding [ 37] for HAR. The method trains a model by comparing a feature map\\nobtained from sensor data with a predicted feature map. Ma et al. [ 29] proposed a loss\\nfunction that uses a reconstruction error, a neighborhood of manually designed features,\\nand a neighborhood of time series data to use an auto-encoder (AE). Tonekaboni et al. [ 13]\\nintroduced the concept of a neighborhood with stationary time series data to propose a\\nneighborhood-based unsupervised learning method for non-stationary multivariate time\\nseries data. These methods utilize the neighborhood of time series data. We also used the\\nneighborhood of time series data as the segment data to acquire the feature representations\\nfor HAR.\\n3. Proposed Method\\n3.1. Data-Measurement Method\\nFigure 1a,b show two typical measurement methods for HAR. The prior-labeling\\ntype in Figure 1a shows that an activity is labeled before the subject initiates an activity.'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 3}, page_content='Sensors 2023 ,23, 8449 4 of 15\\nFor example, to start the walking measurement, the subject presses the “walk” button.\\nThen, to start the running measurement, the subject presses the “run” button. Therefore,\\nin the prior-labeling type (Figure 1a), subjects need to select the activity label before the\\nmeasurement each time. The post-labeling type in Figure 1b shows that an activity is\\nlabeled after the subject initiates an activity. For example, the subjects move freely with the\\nattached sensor while his/her behavior is recorded by video. After the measurement, the\\nrecorded video is visually checked, and each activity is labeled at the given timestamp.\\nsegment datarun staystairs\\nupstep 1\\ndesign and annotationstep 2\\nmeasurement\\nlabel activity\\n(a) Prior-labeling type.\\nsequential data\\nrun staystairs\\nupstep 1\\ndesign and measurementstep 2\\nannotation\\nlabel activity (b) Post-labeling type.\\nsegment datarun staystairs\\nupstep 1\\ndesignstep 2\\nmeasurement\\nlabel activity (c) No label segmentation type.\\nFigure 1. Data collection methods used in sensor-based human activity recognition (HAR).\\nThese methods have advantages and disadvantages. Comparing the prior-labeling\\ntype (Figure 1a) and the post-labeling type (Figure 1b), the prior-labeling type requires\\nsubjects to select an activity label before and after the measurement and is measured for\\neach activity. On the other hand, the post-labeling type does not require the selection of\\nan activity label. In addition, the post-labeling type can continuously measure a series\\nof activities, which reduces the amount of work before and after the measurement, thus\\nreducing the subject’s burden compared to the prior-labeling type. In the prior-labeling\\ntype, an annotator does not need to perform any work after the measurement because the\\nactivity label is selected before the measurement. In the post-labeling type, the annotator\\nmust manually assign the activity label corresponding to the measurement data. As\\nspeciﬁc examples, Kwapisz et al. [ 38] and Zhang et al. [ 39] used the prior-labeling type:\\nIchino et al. [40,41] and Chavarriaga et al. [ 42] used the post-labeling type for measurement.\\nThe aim of our method was to acquire feature representations of single activities. The\\nsingle activity is a motion that consists of a single movement, such as jumping, sitting, or\\nlying down, or a cycle of several consecutive movements, such as walking, running, or\\nclimbing stairs [ 43]. We call data that record one continuous single activity a segment. Our\\nmethod does not require the annotation of the segment data.\\nIn our proposed method, we used a dataset that can be created with the unlabeled\\nsegmented measurement type shown in Figure 1c. This measurement method enables\\nautomatic segmentation of measurement data and eliminates the need for annotation\\nwork from the measurement methods used in related studies [ 44,45]. Compared to the\\ntwo measurement types, the unlabeled segmented measurement type only requires the\\nsubject to indicate the start and end of the measurement, which can be any activity in the\\nsegmented data.\\n3.2. Preprocessing\\nWe utilized unlabeled segment data for unsupervised learning. We used benchmark\\ndatasets in which the sensor data are stored as segment data. In the case of a benchmark\\ndataset that does not have stored segment data, we need to create segment data using\\nsegmentation methods [35,36].\\nA training dataset Dconsists of input data created from segment data and pseudo-label\\npairs. The input data are instances of segment data divided by a sliding window method\\nwith a window size Land a stride width S. We show the creation method of pseudo-labels\\nin Figure 2. The pseudo-labels are unique values assigned to the segment data and are\\ncalled segment labels. On the other hand, pseudo-labels assigned to instance data are called'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 4}, page_content='Sensors 2023 ,23, 8449 5 of 15\\ninstance labels. A total number of instance labels and segment labels, donated by Nand M,\\nrespectively, has a relation M≤N.\\nsegment data instance data\\ninstance label\\nsegment label\\nFigure 2. The difference in assigning pseudo-labels used in instance discrimination (ID) and segment\\ndiscrimination (SD). An instance label (upper) is assigned to the input data in ID. A segment label\\n(lower) is assigned to the segment data in the proposed SD. Therefore, the same segment label can be\\nassigned to different input data.\\n3.3. Model Training Method\\nFigure 3 shows an overview of the proposed method utilizing segment data. In the\\nproposed method, the model consists of an encoder feand two projectors fp1,fp2. The\\nencoder is used to acquire a feature representation of segment data, and the projectors are\\nused for segment label prediction and contrastive loss calculation. We updated the model\\nparameters by using loss functions in SimCLR and SDFD.\\nSimCLR Loss\\n()\\nSDFD Loss\\n( )\\nEncoder\\nsegment instance\\npseudo label\\nConcatProjector 1 Projector 2\\nFigure 3. Overview of the proposed unsupervised learning method. We refer to the proposed\\nsegment-based SimCLR with segment discrimination and feature decorrelation as SimCLR (seg) +\\nSDFD. For the model input, instance data are created from segment data upon which the model is\\nthen trained using the proposed loss function.\\n3.3.1. Segment Discrimination\\nSegment discrimination (SD) is an ID-based method that differs from ID in the assign-\\nment of pseudo-labels. ID is an unsupervised learning method proposed by Wu et al. [ 33].\\nThe SD method uses pseudo-labels like ID, but it uses segment labels instead of instance\\nlabels, as shown in Figure 2. In the SD implementation, the memory bank [ 33] size is the\\nsame as the number of segment labels. The memory bank is a set of normalized feature'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 5}, page_content='Sensors 2023 ,23, 8449 6 of 15\\nrepresentations. Meanwhile, for SD, a model is trained in the same manner as ID using a\\nsegment label instead of an instance label. When the number of segments used in SD is M,\\nthe memory bank is {f1,f2,. . .,fM}. When the feature f=fp2(fe(x))obtained from the\\ninput data xand the segment label is s, the probability P(s|f)is expressed as follows:\\nP(s|f) =exp(f⊤\\nsf)\\n∑S\\nj=1exp(f⊤\\njf). (1)\\nIn SD, it is known that the i-th input data xiis the si-th segment. Therefore, the loss\\nfunction Lsdusing the feature in the memory bank is\\nLsd=−N\\n∑\\ni=1logP(si|fi) =−N\\n∑\\ni=1logexp(f⊤\\nsif)\\n∑S\\nj=1exp(f⊤\\njf). (2)\\nThere are two advantages to using SD. The ﬁrst is that the number of output dimen-\\nsions can be ﬁxed. The total number of segment labels is less than the total number of\\ninstance labels. However, compared to the output size of a typical classiﬁcation model, it\\ncan be extremely large. SD is solved in the same way as ID. Second, it is possible to utilize\\ninformation implicit in the segments. The assigned segment labels allow input data based\\non the segment data to output the same features as the same activity. This is expected to be\\nrobust to phase differences between the input data.\\n3.3.2. SD and Feature Decorrelation\\nSD and feature decorrelation (SDFD) is a method using SD for IDFD as proposed by\\nTao et al. [ 32]. The loss function Lid f dused in IDFD is a combination of the loss function Lid\\nused in ID and a loss function Lf dcalled feature-independent softmax (FIS). Since SDFD\\nis based on SD, the loss function of SDFD is Lsd f d=Lsd+Lf d, which changes the ID loss\\nfunction part from the IDFD loss function Lid f d=Lid+Lf d.\\nFIS is a soft orthogonal constraint on the feature representation that allows non-\\nzeros. Tao et al. [ 32] demonstrated the learning stability advantage of using FIS over an\\nordinary orthogonal constraint. Let us denote the feature representation for each batch\\nF= [f1,f2,. . .,fb]as the dimension of the feature representation d, the batch size b, and the\\ni-th feature representation fi.V=F⊤= [v1,v2,. . .,vd]is used, where the FIS loss function\\nLf dis as follows:\\nLf d=−d\\n∑\\ni=1logexp(v⊤\\nivi)\\n∑d\\nj=1exp(v⊤\\njvi). (3)\\nWe expect that the use of FIS in IDFD and SDFD will make the elements of the feature\\nrepresentation non-correlated.\\n3.3.3. Segment-Based SimCLR\\nSimCLR (seg) is the segment-based SimCLR, which uses a segment to create positive\\npairs. Figure 4 shows the difference in how positive pairs are created. When SimCLR is\\napplied to HAR, the input positive pairs are created from instance data divided by the\\nwindow size. In contrast, SimCLR (seg) positive pairs are created by selecting two instance\\ndata from the same segment.\\nIn SimCLR (seg), the loss function is the same as the loss function used in SimCLR.\\nA positive pair consists of two instance data, xiand xj, which is created from the same\\nsegment. Let tand t′be two data augmentation methods and f(x) = fp1(fe(x))be the\\nmodel to be trained. The feature representations of the two positive pairs used in the'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 6}, page_content='Sensors 2023 ,23, 8449 7 of 15\\nSimCLR loss function are zi=f(t(xi))and zj=f(t′(xj)). Therefore, the SimCLR loss\\nfunction Lsis expressed using the feature representations of the positive pairs ziand zjas\\nLs=−logexp(sim(zi,zj)\\nτ)\\n∑2b\\nk=11(k̸=i)exp(sim(zi,zk)\\nτ). (4)\\nHere, τis the temperature parameter, bis the batch size, 1is the indicator function,\\nand sim is the cosine similarity. We used τ=1for simple cross-entropy in this study.\\nFeature representations are expected to be acquired in a measured activity by creating\\npositive pairs based on segments.\\ndata\\naugmentation Encoder\\ndata\\naugmentation comparison\\n(a) Normal.\\nEncodercomparison\\ndata\\naugmentation data\\naugmentation (b) Ours.\\nFigure 4. Difference in how positive pairs are created as follows: ( a) Normal SimCLR creates positive\\npairs on an instance. ( b) Segment-based SimCLR creates positive pairs from a segment.\\n3.3.4. Segment-Based SimCLR with SDFD\\nWe introduce a method that combines SDFD and SimCLR (seg). The input data are\\ncreated in the same manner as SimCLR (seg). Since an SDFD loss function does not require\\na data pair when passing the data to the loss function, the pair is combined and passed as\\none batch. When combined, the loss function Lis:\\nL=λ1Ls+λ2Lsd+λ3Lf d. (5)\\nIn this loss function, λ1,λ2, and λ3denote the weights of each respective loss function.\\nWe set λ2=λ3=1following the work [ 32] and set λ1to 1. Since tuning the hyperpa-\\nrameters is time-consuming [ 46,47], it may be possible to improve accuracy by adjusting\\nthese parameters.\\nWe discuss the computational complexity of the loss function of the proposed method.\\nThe computational complexity of the loss function for SimCLR (seg) is the same as that of\\nSimCLR, since the only change is in the data-selection method. When changing from IDFD\\nto SDFD, the computational complexity per batch is reduced by O(bd(N−M))because\\nthe memory bank is treated as a matrix. Since SimCLR outputs twice as many feature maps\\nas the batch size in a batch, the computational complexity of the proposed method is the\\nsum of the computational complexity of SimCLR and twice the computational complexity\\nof SDFD.'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 7}, page_content='Sensors 2023 ,23, 8449 8 of 15\\n4. Experiments\\n4.1. Experimental Setup\\nWe evaluated the proposed method on ﬁve benchmark datasets for HAR (HASC [ 40],\\nWisdm [ 38], Opportunity [ 42], UCI-HAR [ 48], and USC-HAD [ 39]). We preprocessed these\\nbenchmark datasets into datasets composed of input data and corresponding segment\\nlabels. Because the HASC [ 40] dataset contains multiple sensor devices, we used only\\nmeasurement data with a sampling frequency of 100 Hz from Apple devices in this ex-\\nperiment. UCI-HAR [ 48] was divided in advance with a window size of 120 and a stride\\nwidth of 60. In order to apply the proposed method, segment labels are required. Therefore,\\nUCI-HAR [ 48] segment information was inferred by comparing the data to create segment\\nlabels. In this study, it was assumed that the change point ground truth information is\\nknown in order to focus on segment utilization effectiveness. We used only accelerometer\\ndata since this is common to each dataset.\\nWe compared the proposed method and baseline methods with a shallow CNN and\\nMLP as a backbone model. The baseline methods were ID [ 33], IDFD [ 32], SimCLR [ 18],\\nand BYOL [ 34]. ID [ 33] and IDFD [ 32] are self-supervised learning methods that classify\\ninstance feature representations, with IDFD [ 32] incorporating an approach where feature\\nrepresentations become uncorrelated from the ID [ 33]. SimCLR [ 18] and BYOL [ 34] are\\ncontrastive learning methods that perform model training by comparing feature represen-\\ntations. While SimCLR [ 18] requires positive and negative pairs to train, BYOL [ 34] is an\\nextension of SimCLR to allow training with positive pairs only. For the backbones of these\\nmethods, we used a three-layer CNN as an encoder and a two-layer multi-layer perceptron\\n(MLP) for the projectors. In the case of BYOL [ 34], the projector and predictor had a Batch-\\nNorm1d in two-layer MLPs. We set the number of output dimensions of the encoder to 128\\nand the number of input and output dimensions of all projectors and predictors to 128.\\nWe investigated the generalization performance of feature representations acquired\\nin few-shot transfer learning (TL) scenarios. In our experiments, we initially constructed\\na dataset consisting of pairs of segment labels and instance data for training, validation,\\nand testing from benchmark datasets. We split the training, validation, and testing datasets\\nby randomly selecting subjects for each dataset. Table 1 shows the number of subjects in\\neach dataset. We then trained an encoder using an unsupervised learning method with\\nthe training dataset as a pretext task. Finally, we performed TL by selecting a subset of the\\ntraining dataset. We assessed the generalization performance of the feature representations\\nby comparing the f1-score when classiﬁed. During TL, we ﬁxed the weight of the encoder\\nand trained only the linear layer attached to the end of the encoder.\\nTable 1. The number of subjects in each dataset. HASC [40] used a limited number of individuals.\\nDataset Training Validation Testing\\nHASC [40] 80 20 30\\nWisdm [38] 21 6 9\\nOpportunity [42] 2 1 1\\nUCI-HAR [48] 19 5 6\\nUSC-HAD [39] 8 2 4\\nWe used a partially different training setup for the pretext task and TL. For the pretext\\ntask, we set a batch size of 64 for Opportunity [ 42] and Wisdm [ 38] and a batch size of 256\\nfor the other datasets. We used an optimization function, Adam, with a learning rate of 0.01\\nand 150 epochs. As data augmentations [ 49], we applied jitter, scaling, and permutation\\nfor UCI-HAR [ 48] and USC-HAD [ 39] and jitter, scaling, and rotate for the other datasets.\\nWe used the weights that resulted in the lowest learning loss during training. For TL, we\\nused a batch size of 512 for UCI-HAR [ 48] and HASC [ 40] and 256 for the other datasets.\\nWe trained the linear layer with a learning rate of 0.001 and 300 epochs without data\\naugmentations. We used the weights with the lowest validation loss for the evaluation. We\\nset the other settings the same as for the pretext task.'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 8}, page_content='Sensors 2023 ,23, 8449 9 of 15\\nThe window size selected has room for discussion. Based on the selected window size\\nand the sampling frequency of the respective dataset, the duration for the utilized instances\\nranged from a minimum of 2.56 s to a maximum of 6.4 s. Speciﬁcally, for the HASC dataset,\\neach instance had a duration of 2.56 s. In addition, it is highly probable that at least a single\\nmovement is included, considering the fact that the single movements are at intervals of\\n1–2 s [ 43]. Therefore, the experimental environment was considered to have no lack of\\nactivity features in a single instance. We conﬁrmed that the proposed method was able to\\nacquire a feature representation of the activity in at least such an environment.\\n4.2. Ten-Shot Transfer Learning Veriﬁcation\\nWe tested the effectiveness of the feature representation acquired in the pretext task\\nby performing transfer learning (TL) with 10-shot data per class. In this experiment, we\\nrandomly selected 10-shot data from each activity in the training dataset and trained the\\nlinear layer by TL. Table 2 shows the average f1-scores for ﬁve trials. The Non-pre in Table 2\\nshows the results of training the entire model with initial weights on the selected 10-shots\\nwithout a pretext task. The proposed SimCLR (seg) + SDFD achieved 70.12%, 76.36%, and\\n91.08% on the HASC, Wisdm, and UCI-HAR, respectively. In particular, the HASC results\\nshowed a 7.26% improvement in the f1-score compared to SimCLR, which does not focus\\non segments. We considered that the reason for the relatively high score of Non-pre in the\\nresults of Wisdm and UCI-HAR is that we used a small model and the training dataset\\nwas sufﬁcient to train the entire model for classiﬁcation. These results showed that the\\nproposed method can effectively acquire feature representations in unsupervised learning\\nmethods using sensor data.\\nTable 2. Average f1-score of each dataset in the same-domain transfer learning (TL). The encoder\\nweights are ﬁxed therein, and the number of instances is N=10. The bold font indicates the highest\\nscore, while the underlined font is the second-highest among the datasets used.\\nHASC Wisdm UCI-HAR\\nNon-pre 0.3714 0.7286 0.9397\\nBYOL [34] 0.6072 0.5676 0.7198\\nSimCLR [18] 0.6287 0.6717 0.8492\\nID [33] 0.2354 0.6661 0.8926\\nIDFD [32] 0.6195 0.6962 0.8986\\nSimCLR (seg) + SDFD (ours) 0.7012 0.7636 0.9108\\nTable 3 shows the results of the ablation study. Comparing IDFD and SDFD, the\\nWisdm results showed an f1-score improvement of 10.18%. The SimCLR (seg) results\\nshowed an improvement from the SimCLR results by 6.62%, 9.62%, and 4.94% for HASC,\\nWisdm, and UCI-HAR, respectively. The results for SDFD and SimCLR (seg) showed\\nthat SimCLR (seg) was better for HASC and SDFD was better for Wisdm and UCI-HAR.\\nSimCLR (seg) + SDFD outperformed SimCLR (seg) and SDFD by 0.64% and 0.61% for\\nHASC and UCI-HAR, respectively. SimCLR (seg) + SDFD was 0.43% lower than SimCLR\\n(seg) in Wisdm. The results showed that introducing the segment-based method improved\\nthe f1-score. Considering the training dataset size, SimCLR (seg) requires a larger number\\nof subjects than SDFD to acquire feature representations, but its combination with SDFD\\nmay have made it possible to acquire feature representations with a relatively small num-\\nber of subjects. Considering the effectiveness of FIS and segment-based approaches, we\\nconsidered it important to group similar features in unsupervised HAR in order to acquire\\nmore-generalized feature representations.\\n4.3. Confusion Matrix\\nFigure 5 shows the confusion matrix resulting from TL using 10-shot data for each\\nclass in HASC. We presented the confusion matrix using the most-accurate results from the\\nﬁve trials. The HASC dataset contains six activities: “stay”, “walk”, “jog”, “skip”, “stUp”'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 9}, page_content='Sensors 2023 ,23, 8449 10 of 15\\n(stair up), and “stDown” (stair down). The confusion matrix revealed that “walk”, “stUp”,\\nand “stDown” were frequently misclassiﬁed. Compared to these results, the proposed\\nmethod made more errors for “stUp”, but improved the prediction of “walk” and “stDown”.\\nTable 4 shows the average f1-scores for each activity. The results indicate that the proposed\\nmethod had higher scores for “walk”, “jog”, “skip”, and “stDown”, but lower scores for\\n“stay” and “stUp” compared to other methods. SDFD and SimCLR (seg) showed similar\\nor higher scores for most activities compared to IDFD and SimCLR, respectively. The\\nproposed method achieved the highest f1-scores of 87.38% and 82.68% for “jog” and “skip”,\\nrespectively. The proposed method improved the scores of “walk” and “stDown” by up to\\n16.23% and 11.93%, respectively, and decreased the scores of “stUp” by 1.90% compared to\\nIDFD and SimCLR. From these results, we considered that the proposed method achieved\\na better separation of the “walk” and “stDown” feature representations, but brought the\\n“walk” and “stUp” feature representations closer together. Furthermore, based on the\\nconfusion matrix results, it can be considered that the proposed method mitigated the bias\\nin predicting “stUp”.\\nTable 3. Ablation study. The bold and underlined fonts indicate the highest and second-highest\\nf1-score, respectively.\\nHASC Wisdm UCI-HAR\\nIDFD [32] 0.6195 0.6962 0.8986\\nSDFD 0.6875 0.7980 0.9047\\nSimCLR [18] 0.6287 0.6717 0.8492\\nSimCLR (seg) 0.6948 0.7679 0.8986\\nSimCLR (seg) + SDFD 0.7012 0.7636 0.9108\\nstay walk jog skip stUp stDown\\nPredicted labelstay walk jog skip stUp stDownTrue label583\\n100.00%0\\n0.00%0\\n0.00%0\\n0.00%0\\n0.00%0\\n0.00%\\n32\\n5.54%201\\n34.78%0\\n0.00%0\\n0.00%315\\n54.50%30\\n5.19%\\n0\\n0.00%7\\n1.18%493\\n82.86%32\\n5.38%49\\n8.24%14\\n2.35%\\n0\\n0.00%12\\n2.05%70\\n11.99%413\\n70.72%51\\n8.73%38\\n6.51%\\n40\\n6.42%76\\n12.20%6\\n0.96%2\\n0.32%481\\n77.21%18\\n2.89%\\n10\\n1.61%85\\n13.69%2\\n0.32%1\\n0.16%258\\n41.55%265\\n42.67%\\n0%20%40%60%80%100%\\n(a) SimCLR.\\nstay walk jog skip stUp stDown\\nPredicted labelstay walk jog skip stUp stDownTrue label583\\n100.00%0\\n0.00%0\\n0.00%0\\n0.00%0\\n0.00%0\\n0.00%\\n43\\n7.44%421\\n72.84%0\\n0.00%0\\n0.00%101\\n17.47%13\\n2.25%\\n0\\n0.00%12\\n2.02%549\\n92.27%20\\n3.36%14\\n2.35%0\\n0.00%\\n0\\n0.00%0\\n0.00%43\\n7.36%516\\n88.36%11\\n1.88%14\\n2.40%\\n53\\n8.51%278\\n44.62%14\\n2.25%3\\n0.48%255\\n40.93%20\\n3.21%\\n16\\n2.58%132\\n21.26%1\\n0.16%2\\n0.32%69\\n11.11%401\\n64.57%\\n0%20%40%60%80%100% (b) SimCLR (seg) + SDFD.\\nFigure 5. Confusion matrix with the number of instances and percentages. The axes represent the\\nactivities included in HASC. In particular, “stUp” represents “stair up”, and “stDown” represents\\n“stair down”.\\nTable 4. f1-scores by activity for the HASC when TL with 10-shots per class. The bolded font indicates\\nthe most-accurate values.\\nStay Walk Jog Skip stUp stDown\\nIDFD [32] 0.8970 0.3520 0.7752 0.6862 0.5017 0.5048\\nSimCLR [18] 0.8992 0.3916 0.7682 0.7092 0.4895 0.5147\\nSDFD 0.8977 0.4470 0.8548 0.8183 0.5170 0.5905\\nSimCLR (seg) 0.8756 0.4942 0.8525 0.8213 0.5432 0.5819\\nSimCLR (seg) + SDFD 0.8857 0.5143 0.8738 0.8268 0.4827 0.6241'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 10}, page_content='Sensors 2023 ,23, 8449 11 of 15\\n4.4. Instance Effectiveness for Few-Shot TL\\nWe investigated the effect of the number of instances in few-shot TL. We compared\\nthe average f1-scores of ﬁve trials by performing TL on randomly selected few-shot data\\nper class. Figure 6 shows the average f1-scores for ﬁve trials when only the linear layer\\nwas trained. The legend represents methods with pretext tasks, and Non-pre represents a\\nmethod that trained the entire model without pretext tasks. All methods tended to have\\nhigher f1-score as the number of instances increased. When the number of instances was\\nﬁve and ten, Non-pre showed low values at 33.41% and 37.14%, respectively. Compared\\nto Non-pre, the models trained with pre-training showed high f1-scores. In particular,\\nSimCLR (seg) and SimCLR (seg) + SDFD showed relatively high f1-scores at 62.02% and\\n60.02%, respectively, for ﬁve-shot TL. As the number of labels increased, the f1-score of\\nSDFD and SimCLR (seg) + SDFD also increased. SimCLR (seg) and SDFD showed relatively\\nhigh f1-scores when the number of labeled data was 5 and 100, respectively.\\nThe combination of SimCLR (seg) and SDFD improved the f1-score of SDFD in TL\\nwith extremely small numbers of labels.\\n/uni00000018 /uni00000014/uni00000013 /uni00000015/uni00000018 /uni00000018/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000029/uni00000014/uni00000003/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048\\n/uni00000036/uni00000027/uni00000029/uni00000027\\n/uni00000036/uni0000004c/uni00000050/uni00000026/uni0000002f/uni00000035/uni0000000b/uni00000056/uni00000048/uni0000004a/uni0000000c\\n/uni00000036/uni0000004c/uni00000050/uni00000026/uni0000002f/uni00000035/uni0000000b/uni00000056/uni00000048/uni0000004a/uni0000000c/uni0000000e/uni00000036/uni00000027/uni00000029/uni00000027\\n/uni0000002c/uni00000027/uni00000029/uni00000027\\n/uni00000036/uni0000004c/uni00000050/uni00000026/uni0000002f/uni00000035\\n/uni00000031/uni00000052/uni00000051/uni00000010/uni00000053/uni00000055/uni00000048\\nFigure 6. The number of labeled data and the f1-score of each model. The horizontal axis indicates\\nthe number of instances.\\n4.5. Effectiveness for Different Domains\\nWe investigated whether the proposed method can acquire a generalized feature\\nrepresentation to different domains. In this experiment, we performed TL on a training\\ndataset (target dataset) created from a different benchmark dataset than the dataset used in\\nthe pretext task (pretext dataset). Figure 7 shows the results of training on the target dataset,\\nwhere the horizontal axis is labeled “pretext dataset →target dataset”. When transferred\\nfrom HASC, SimCLR (seg) + SDFD showed a relatively high f1-score. In particular, when\\ntransferred to UCI-HAR, SimCLR (seg) + SDFD achieved 91.82%. When transferred from\\nUSC-HAD to HASC, SDFD had the highest f1-score of 66.01%. In the results of TL for\\ndifferent domains from USC-HAD to HASC, the SD-based methods showed a higher\\nf1-score, while methods using SimCLR showed a lower score. The number of subjects\\nin the USC-HAD training dataset was small compared to HASC and Wisdm. Therefore,\\nwe considered the lower f1-score of the SimCLR-based method caused by the smaller\\ntotal number of training data, which prevented the model from obtaining a generalized\\nfeature representation. When the size of the dataset was not large enough during the\\npretext task, we assumed that the SDFD-based method was an effective approach to obtain\\ngeneralized feature representations. In addition, since HASC, Wisdm, and UCI-HAR all\\nhave different sampling frequencies, we considered SimCLR (seg) + SDFD to be robust to\\ndifferent sampling frequencies when there was a sufﬁcient amount of training data for the\\npretext task.'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 11}, page_content='Sensors 2023 ,23, 8449 12 of 15\\n/uni00000058/uni00000056/uni00000046/uni0000004b/uni00000044/uni00000047/uni0000004b/uni00000044/uni00000056/uni00000046\\n/uni0000004b/uni00000044/uni00000056/uni00000046/uni00000052/uni00000053/uni00000053/uni00000052/uni00000055/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000057/uni0000005c\\n /uni0000004b/uni00000044/uni00000056/uni00000046/uni00000058/uni00000046/uni0000004c\\n/uni0000004b/uni00000044/uni00000056/uni00000046/uni00000058/uni00000056/uni00000046/uni0000004b/uni00000044/uni00000047\\n/uni0000004b/uni00000044/uni00000056/uni00000046/uni0000005a/uni0000004c/uni00000056/uni00000047/uni00000050\\n/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013/uni00000029/uni00000014/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048/uni00000036/uni00000027\\n/uni00000036/uni00000027/uni00000029/uni00000027\\n/uni00000036/uni0000004c/uni00000050/uni00000026/uni0000002f/uni00000035/uni0000000b/uni00000056/uni00000048/uni0000004a/uni0000000c\\n/uni00000036/uni0000004c/uni00000050/uni00000026/uni0000002f/uni00000035/uni0000000b/uni00000056/uni00000048/uni0000004a/uni0000000c/uni0000000e/uni00000036/uni00000027/uni00000029/uni00000027\\n/uni0000002c/uni00000027/uni00000029/uni00000027\\n/uni00000036/uni0000004c/uni00000050/uni00000026/uni0000002f/uni00000035\\nFigure 7. Transfer performance to different datasets. The x-axis indicates “pretext dataset →target\\ndataset”.\\n5. Conclusions\\nIn this study, we tackled developing a new unsupervised representation learning\\nmethod using segment information for HAR. We explained the measurement methods\\nfor sensor data to construct a HAR dataset. We introduced segment data, which are\\nmeasurement data containing only one activity without annotating an activity label. The\\ndataset used in our method consisted of instance data and segment labels, which were\\npseudo-labels assigned to the segment data. We proposed the new unsupervised learning\\nmethod SimCLR (seg) + SDFD by combining SimCLR (seg) and SDFD. To investigate the\\neffectiveness of the feature representation obtained by the proposed method, we conducted\\nexperiments by training only a linear layer attached at the end of an encoder with ﬁxed\\nencoder weights as TL. The experimental results showed that our proposed segment-based\\nSimCLR methods obtained more-generalized feature representations with a higher f1-score\\nthan the regular SimCLR in HAR. We applied TL to different domains and found that the\\nproposed SimCLR (seg) + SDFD performed robustly with respect to the sampling frequency\\nof the sensor data. From these experiments, we suggest that the SimCLR-based method\\nrequires more training data than SDFD for the pretext task, but is effective when there are\\nsufﬁcient unlabeled data.\\nHowever, there are several limitations to consider. Firstly, this study used only ac-\\ncelerometer data for the datasets and did not discuss the quantity or other sensor data such\\nas gyroscope or magnetometer data. Therefore, it can be expected that the accuracy of the\\nproposed method will be improved by multimodalization. Secondly, this study only used a\\ndataset with basic activities. It is expected that more-generalized feature representations\\ncould be obtained by using a dataset with a wider range of activities. Third, this study did\\nnot use a method to split the accelerometer data containing multiple activities into segment\\ndata. As future work, we will introduce multimodalization in the proposed method, us-\\ning datasets with a wider range of activities and enhancing a framework that includes a\\nmethod for splitting accelerometer data with multiple activities into segment data. These\\napproaches will lead to more-accurate and -effective HAR systems.\\nAuthor Contributions: Conceptualization, K.T., K.K. and T.H.; Methodology, K.T., K.K. and T.H.;\\nSoftware, K.T. and K.K.; Validation, K.T.; Investigation, K.T.; Writing—original draft, K.T.; Writing—\\nreview & editing, K.T. and T.H.; Visualization, K.T.; Supervision, T.H.; Project administration, T.H.\\nAll authors have read and agreed to the published version of the manuscript.\\nFunding: This work was supported in part by the Japan Society for the Promotion of Science (JSPS)\\nKAKENHI Grant-in-Aid for Early-Career Scientists and Grant-in-Aid for Scientiﬁc Research (C)\\nunder Grants 19K20420 and 23K11164.\\nInstitutional Review Board Statement: Not applicable.'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 12}, page_content='Sensors 2023 ,23, 8449 13 of 15\\nInformed Consent Statement: Not applicable.\\nData Availability Statement: Not applicable.\\nConﬂicts of Interest: The authors declare no conﬂict of interest. The funders had no role in the design\\nof the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or\\nin the decision to publish the results.\\nReferences\\n1. Thiel, D.V .; Sarkar, A.K. Swing Proﬁles in Sport: An Accelerometer Analysis. Procedia Eng. 2014 ,72, 624–629. [CrossRef]\\n2. Klein, M.C.; Manzoor, A.; Mollee, J.S. Active2Gether: A Personalized m-Health Intervention to Encourage Physical Activity.\\nSensors 2017 ,17, 1436. [CrossRef] [PubMed]\\n3. Plangger, K.; Campbell, C.; Robson, K.; Montecchi, M. Little rewards, big changes: Using exercise analytics to motivate sustainable\\nchanges in physical activity. Inf. Manag. 2022 ,59, 103216. [CrossRef]\\n4. Thomas, B.; Lu, M.L.; Jha, R.; Bertrand, J. Machine Learning for Detection and Risk Assessment of Lifting Action. IEEE\\nTrans.-Hum.-Mach. Syst. 2022 ,52, 1196–1204. [CrossRef]\\n5. Zeng, M.; Nguyen, L.T.; Yu, B.; Mengshoel, O.J.; Zhu, J.; Wu, P .; Zhang, J. Convolutional Neural Networks for human activity\\nrecognition using mobile sensors. In Proceedings of the 6th International Conference on Mobile Computing, Applications and\\nServices, Austin, TX, USA, 6–7 November 2014; pp. 197–205.\\n6. Ordóñez, F.J.; Roggen, D. Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity\\nRecognition. Sensors 2016 ,16, 115. [CrossRef] [PubMed]\\n7. Murad, A.; Pyun, J.Y. Deep Recurrent Neural Networks for Human Activity Recognition. Sensors 2017 ,17, 2556. [CrossRef]\\n[PubMed]\\n8. Guinea, A.S.; Sarabchian, M.; Mühlhäuser, M. Improving Wearable-Based Activity Recognition Using Image Representations.\\nSensors 2022 ,22, 1840. [CrossRef] [PubMed]\\n9. Jalal, A.; Quaid, M.A.K.; Hasan, A.S. Wearable Sensor-Based Human Behavior Understanding and Recognition in Daily Life\\nfor Smart Environments. In Proceedings of the 2018 International Conference on Frontiers of Information Technology (FIT),\\nIslamabad, Pakistan, 17–19 December 2018; pp. 105–110.\\n10. Liu., H.; Schultz., T. A Wearable Real-time Human Activity Recognition System using Biosensors Integrated into a Knee Bandage.\\nIn Proceedings of the 12th International Joint Conference on Biomedical Engineering Systems and Technologies (BIOSTEC\\n2019)-BIODEVICES, Prague, Czech Republic, 22–24 February 2019; INSTICC; SciTePress: Setúbal, Portugal, 2019; pp. 47–55.\\n11. Liu, H.; Gamboa, H.; Schultz, T. Sensor-Based Human Activity and Behavior Research: Where Advanced Sensing and Recognition\\nTechnologies Meet. Sensors 2023 ,23, 125. [CrossRef]\\n12. Haresamudram, H.; Essa, I.; Plötz, T. Contrastive Predictive Coding for Human Activity Recognition. ACM Interact. Mob.\\nWearable Ubiquitous Technol. 2021 ,5, 1–26. [CrossRef]\\n13. Tonekaboni, S.; Eytan, D.; Goldenberg, A. Unsupervised Representation Learning for Time Series with Temporal Neighborhood\\nCoding. In Proceedings of the ninth International Conference on Learning Representations, Virtual only, 3–7 May 2021.\\n14. Zerveas, G.; Jayaraman, S.; Patel, D.; Bhamidipaty, A.; Eickhoff, C. A Transformer-Based Framework for Multivariate Time\\nSeries Representation Learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,\\nNew York, NY, USA, 14–18 August 2021; pp. 2114–2124.\\n15. Xiao, Z.; Xing, H.; Zhao, B.; Qu, R.; Luo, S.; Dai, P .; Li, K.; Zhu, Z. Deep Contrastive Representation Learning with Self-Distillation.\\nIEEE Trans. Emerg. Top. Comput. Intell. 2023 , 1–13. [CrossRef]\\n16. Franceschi, J.Y.; Dieuleveut, A.; Jaggi, M. Unsupervised scalable representation learning for multivariate time series. In\\nProceedings of the Advances in Neural Information Processing Systems, Vancouver, Canada, 8–14 December 2019; Wallach, H.,\\nLarochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E., Garnett, R., Eds.; Curran Associates, Inc.: Red Hook, NY, USA, 2019;\\nVolume 32.\\n17. Sohn, K. Improved Deep Metric Learning with Multi-class N-pair Loss Objective. In Proceedings of the Advances in Neural\\nInformation Processing Systems, Barcelona, Spain, 5–10 December 2016; Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., Garnett,\\nR., Eds.; Curran Associates, Inc.: Red Hook, NY, USA, 2016; Volume 29.\\n18. Chen, T.; Kornblith, S.; Norouzi, M.; Hinton, G. A Simple Framework for Contrastive Learning of Visual Representations. In\\nProceedings of the 37th International Conference on Machine Learning, Virtual only, 12–18 July 2020.\\n19. Takenaka, K.; Hasegawa, T. Unsupervised Representation Learning Method In Sensor Based Human Activity Recognition.\\nIn Proceedings of the International Conference on Machine Learning and Cybernetics, Toyama, Japan, 9–11 September 2022;\\npp. 25–30.\\n20. Hartmann, Y.; Liu, H.; Schultz, T. Feature Space Reduction for Multimodal Human Activity Recognition. In Proceedings of the\\n13th International Joint Conference on Biomedical Engineering Systems and Technologies (BIOSTEC 2020)-BIOSIGNALS, Valletta,\\nMalta, 24–26 February 2020; INSTICC; SciTePress: Setúbal, Portugal, 2020; pp. 135–140.\\n21. Liu, H.; Xue, T.; Schultz, T. On a Real Real-Time Wearable Human Activity Recognition System. In Proceedings of the 16th\\nInternational Joint Conference on Biomedical Engineering Systems and Technologies-WHC, Lisbon, Portugal, 16–18 February\\n2023; INSTICC; SciTePress: Setúbal, Portugal, 2023; pp. 711–720.'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 13}, page_content='Sensors 2023 ,23, 8449 14 of 15\\n22. Bento, N.; Rebelo, J.; Barandas, M.; Carreiro, A.V .; Campagner, A.; Cabitza, F.; Gamboa, H. Comparing Handcrafted Features and\\nDeep Neural Representations for Domain Generalization in Human Activity Recognition. Sensors 2022 ,22, 7324. [CrossRef]\\n[PubMed]\\n23. Dirgová Luptáková, I.; Kubovˇ cík, M.; Pospíchal, J. Wearable Sensor-Based Human Activity Recognition with Transformer Model.\\nSensors 2022 ,22, 1911. [CrossRef] [PubMed]\\n24. Mahmud, T.; Sazzad Sayyed, A.Q.M.; Fattah, S.A.; Kung, S.Y. A Novel Multi-Stage Training Approach for Human Activity\\nRecognition From Multimodal Wearable Sensor Data Using Deep Neural Network. IEEE Sens. J. 2021 ,21, 1715–1726. [CrossRef]\\n25. Qian, H.; Pan, S.J.; Da, B.; Miao, C. A Novel Distribution-Embedded Neural Network for Sensor-Based Activity Recognition. In\\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI-19, Macao SAR, China, 10–16\\nAugust 2019; pp. 5614–5620.\\n26. Gao, W.; Zhang, L.; Teng, Q.; He, J.; Wu, H. DanHAR: Dual Attention Network for multimodal human activity recognition using\\nwearable sensors. Appl. Soft Comput. 2021 ,111, 107728. [CrossRef]\\n27. Sozinov, K.; Vlassov, V .; Girdzijauskas, S. Human Activity Recognition Using Federated Learning. In Proceedings of the IEEE Intl\\nConf on Parallel & Distributed Processing with Applications, Ubiquitous Computing & Communications, Big Data & Cloud\\nComputing, Social Computing & Networking, Sustainable Computing & Communications, Melbourne, VIC, Australia, 11–13\\nDecember 2018; pp. 1103–1111.\\n28. Li, C.; Niu, D.; Jiang, B.; Zuo, X.; Yang, J. Meta-HAR: Federated Representation Learning for Human Activity Recognition. In\\nProceedings of the Web Conference 2021, New York, NY, USA, 19–23 April 2021; pp. 912–922.\\n29. Ma, H.; Zhang, Z.; Li, W.; Lu, S. Unsupervised Human Activity Representation Learning with Multi-Task Deep Clustering. Acm\\nInteract. Mob. Wearable Ubiquitous Technol. 2021 ,5, 1–25. [CrossRef]\\n30. He, K.; Fan, H.; Wu, Y.; Xie, S.; Girshick, R. Momentum Contrast for Unsupervised Visual Representation Learning. In\\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 13–19 June 2020;\\npp. 9726–9735.\\n31. Chen, X.; He, K. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, Nashville, TN, USA, 20–25 June 2021; pp. 15750–15758.\\n32. Tao, Y.; Takagi, K.; Nakata, K. Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation.\\nIn Proceedings of the International Conference on Learning Representations, Virtual, 3–7 May 2021.\\n33. Wu, Z.; Xiong, Y.; Yu, S.X.; Lin, D. Unsupervised Feature Learning via Non-Parametric Instance Discrimination. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA, 18–23 June 2018; pp. 3733–3742.\\n34. Grill, J.B.; Strub, F.; Altché, F.; Tallec, C.; Richemond, P .H.; Buchatskaya, E.; Doersch, C.; Pires, B.A.; Guo, Z.D.; Azar, M.G.; et al.\\nBootstrap Your Own Latent a New Approach to Self-Supervised Learning. In Proceedings of the Advances in Neural Information\\nProcessing Systems, Virtual only, 6–12 December 2020; Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin, H., Eds.; Curran\\nAssociates, Inc.: Red Hook, NY, USA, 2020; Volume 33.\\n35. Rodrigues, J.; Liu, H.; Folgado, D.; Belo, D.; Schultz, T.; Gamboa, H. Feature-Based Information Retrieval of Multimodal Biosignals\\nwith a Self-Similarity Matrix: Focus on Automatic Segmentation. Biosensors 2022 ,12, 1182. [CrossRef] [PubMed]\\n36. Folgado, D.; Barandas, M.; Antunes, M.; Nunes, M.L.; Liu, H.; Hartmann, Y.; Schultz, T.; Gamboa, H. TSSEARCH: Time Series\\nSubsequence Search Library. SoftwareX 2022 ,18, 101049. [CrossRef]\\n37. Oord, A.v.d.; Li, Y.; Vinyals, O. Representation learning with contrastive predictive coding. arXiv 2018 , arXiv:1807.03748.\\n38. Kwapisz, J.R.; Weiss, G.M.; Moore, S.A. Activity Recognition Using Cell Phone Accelerometers. Acm Sigkdd Explor. Newsl. 2011 ,\\n12, 74–82. [CrossRef]\\n39. Zhang, M.; Sawchuk, A.A. USC-HAD: A Daily Activity Dataset for Ubiquitous Activity Recognition Using Wearable Sensors. In\\nProceedings of the 2012 ACM Conference on Ubiquitous Computing, New York, NY, USA, 5–8 September 2012; pp. 1036–1043.\\n40. Ichino, H.; Kaji, K.; Sakurada, K.; Hiroi, K.; Kawaguchi, N. HASC-PAC2016: Large Scale Human Pedestrian Activity Corpus\\nand Its Baseline Recognition. In Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous\\nComputing: Adjunct, New York, NY, USA, 12–16 September 2016; pp. 705–714.\\n41. Kawaguchi, N.; Yang, Y.; Yang, T.; Ogawa, N.; Iwasaki, Y.; Kaji, K.; Terada, T.; Murao, K.; Inoue, S.; Kawahara, Y.; et al.\\nHASC2011corpus: Towards the Common Ground of Human Activity Recognition. In Proceedings of the 13th International\\nConference on Ubiquitous Computing, New York, NY, USA, 17–21 September 2011; pp. 571–572.\\n42. Chavarriaga, R.; Sagha, H.; Calatroni, A.; Digumarti, S.T.; Tröster, G.; del R. Millán, J.; Roggen, D. The Opportunity challenge: A\\nbenchmark database for on-body sensor-based activity recognition. Pattern Recognit. Lett. 2013 ,34, 2033–2042. [CrossRef]\\n43. Liu, H.; Schultz, T. How Long Are Various Types of Daily Activities? Statistical Analysis of a Multimodal Wearable Sensor-Based\\nHuman Activity Dataset. In Proceedings of the 15th International Joint Conference on Biomedical Engineering Systems and\\nTechnologies (BIOSTEC 2022)-Volume 5: HEALTHINF, Online Streaming, 9–11 February 2022; pp. 680–688.\\n44. Liu, H.; Schultz, T. ASK: A Framework for Data Acquisition and Activity Recognition. In Proceedings of the International\\nConference on Bio-inspired Systems and Signal Processing, Funchal, Madeira, Portugal, 19–21 January 2018.\\n45. Liu, H.; Hartmann, Y.; Schultz, T. CSL-SHARE: A Multimodal Wearable Sensor-Based Human Activity Dataset. Front. Comput.\\nSci.2021 ,3, 759136. [CrossRef]\\n46. Castro, R.L.; Andrade, D.; Fraguela, B. Reusing Trained Layers of Convolutional Neural Networks to Shorten Hyperparameters\\nTuning Time. arXiv 2020 , arXiv:2006.09083.'),\n",
       " Document(metadata={'source': 'D:\\\\study\\\\RAG_PIPLELINE_WITH_LLAMA_INDEX_LANGCHAIN_GPT_GEMINI\\\\RAG_APPLICATION_USING_MISTRAL_WEVIATE\\\\sensors.pdf', 'page': 14}, page_content='Sensors 2023 ,23, 8449 15 of 15\\n47. Oyelade, O.N.; Ezugwu, A.E. A comparative performance study of random-grid model for hyperparameters selection in detection\\nof abnormalities in digital breast images. Concurr. Comput. Pract. Exp. 2022 ,34, e6914. [CrossRef]\\n48. Anguita, D.; Ghio, A.; Oneto, L.; Parra, X.; Reyes-Ortiz, J.L. A Public Domain Dataset for Human Activity Recognition Using\\nSmartphones. In Proceedings of the European Symposium on Artiﬁcial Neural Networks, Computational Intelligence and\\nMachine Learning, Bruges, Belgium, 24–26 April 2013; pp. 437–442.\\n49. Um, T.T.; Pﬁster, F.M.J.; Pichler, D.; Endo, S.; Lang, M.; Hirche, S.; Fietzek, U.; Kuli´ c, D. Data Augmentation of Wearable Sensor\\nData for Parkinson’s Disease Monitoring Using Convolutional Neural Networks. In Proceedings of the 19th ACM International\\nConference on Multimodal Interaction, New York, NY, USA, 13–17 November 2017; pp. 216–220.\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vector_db = Weaviate.from_documents(\n",
    "    docs, embeddings, client=client, by_text=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activities wherein the data are measured by sensors such as accelerometers and gyroscopes.\n",
      "For example, human activities such as “walking” and “running” were predicted from\n",
      "the measured accelerometer data. This technology is used in various applications from\n",
      "analyzing sports movements [ 1] to healthcare such as health awareness maintenance and\n",
      "the detection of risky movements by patients [ 2–4]. HAR is an essential technology because\n",
      "the predicted results have some inﬂuence on decision-making.\n",
      "Recently, neural networks (NNs) have been used for HAR [ 5–8]. HAR is typically\n",
      "implemented using machine learning methods [ 9,10] such as support vector machines\n",
      "(SVMs) and hidden Markov models (HMMs). Machine learning models require a training\n",
      "dataset consisting of handcrafted features extracted from sensor data and corresponding\n",
      "activity labels. In contrast to traditional machine learning, deep learning methods such\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\n",
    "    vector_db.similarity_search(\n",
    "        \"what is HAR?\", k=3)[1].page_content\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template=\"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use ten sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "     \n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(template)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks.\\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse ten sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\\n\"))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now in need llm model by huggingface we access using huggingface pipeline or using apikey\n",
    "huggingkey = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install -U langchain-huggingface\n",
    "import os\n",
    "os.environ['huggingkey'] = huggingkey\n",
    "os.environ['huggingkey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('huggingkey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "model = HuggingFaceHub(\n",
    "    huggingfacehub_api_token = huggingkey,\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    model_kwargs={'temperature':1,\"max_length\":180}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parse = StrOutputParser() # create the object of the parser class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrivers = vector_db.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {'context':retrivers,\n",
    "     'question':RunnablePassthrough()}\n",
    "     | prompt | model |output_parse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = rag_chain.invoke(\"what is human activity prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human activity prediction is a technology that uses sensors to measure human activities such as walking and running. It is used in various applications such as sports movements analysis and healthcare. The predicted results have some influence on decision-making. Neural networks (NNs) have been used for human activity recognition (HAR) recently. HAR is typically implemented using machine learning methods such as support vector machines (SVMs) and hidden Markov models (HMMs). Machine learning models require a training'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\"\\n\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
